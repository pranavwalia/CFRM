{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual Regret Minimization Implementation for Rock Paper Scissors\n",
    "\n",
    "Counterfactual regret minimization is an important concept in algorithmic game theory. It has made the creation of super-human poker AI possible and is fundamental for solving games of imperfect information.\n",
    "\n",
    "\n",
    "The algorithm can be broadly described as follows:\n",
    "\n",
    "**For Some Number of Iterations:**\n",
    "   * Compute a regret-matching strategy\n",
    "   * Add strategy profile to the profile sum\n",
    "   * Select each player action profile according to the strategy profile\n",
    "   * Compute regrets\n",
    "   * Add player regrets to player cumulative regrets\n",
    "   * Return the average strategy profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rock Paper Scissors\n",
    "\n",
    "### Key Variables:\n",
    "\n",
    "* Indices to acess options in strategy list\n",
    "* regretSum: A list to keep track of the total regret of each decision\n",
    "* strategy: A list to hold the weightings of each option in a mixed strategy\n",
    "* strategySum: The sum of all the strategies used thus far\n",
    "* oppStrategy: The strategy of a theoretical opponent against whom we must adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regret Matching\n",
    " * Computes the strategy as the accumulated regrets / total regret\n",
    " * Computes the strategySum as the given strategySum + the newly derived strategy based on accumulated regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getStrategy(regretSum,strategySum):\n",
    "    actions = 3\n",
    "    normalizingSum = 0\n",
    "    strategy = [0,0,0]\n",
    "    #Normalizingsum is the sum of positive regrets\n",
    "    for i in range(0,actions):\n",
    "        if regretSum[i] > 0:\n",
    "            strategy[i] = regretSum[i]\n",
    "        else:\n",
    "            strategy[i] = 0\n",
    "        normalizingSum += strategy[i]\n",
    "        \n",
    "    for i in range(0,actions):\n",
    "        if normalizingSum > 0:\n",
    "            strategy[i] = strategy[i]/normalizingSum\n",
    "        else:\n",
    "            strategy[i] = 1.0 / actions\n",
    "        strategySum[i] += strategy[i]\n",
    "    return (strategy,strategySum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull A Random Action According To Current Mixed Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a random action according to the strategy\n",
    "def getAction(strategy):\n",
    "    r = random.uniform(0,1)\n",
    "    if r >= 0 and r < strategy[0]:\n",
    "        return 0\n",
    "    elif r >= strategy[0] and r < strategy[0] + strategy[1]:\n",
    "        return 1\n",
    "    elif r >= strategy[0] + strategy[1] and r < sum(strategy):\n",
    "        return 2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Algorithm\n",
    "- Accumulate regretSums after every round\n",
    "- Compute a regret-matching strategy based on those regret sums\n",
    "- Add Strategy to the sum of all the previously computed profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations,regretSum,oppStrategy):\n",
    "    actionUtility = [0,0,0]\n",
    "    strategySum = [0,0,0]\n",
    "    actions = 3\n",
    "    for i in range(0,iterations):\n",
    "        \n",
    "        ##Retrieve Actions\n",
    "        t = getStrategy(regretSum,strategySum)\n",
    "        strategy = t[0]\n",
    "        strategySum = t[1]\n",
    "        #print(strategy)\n",
    "        myaction = getAction(strategy)\n",
    "        #Define an arbitrary opponent strategy from which to adjust\n",
    "        otherAction = getAction(oppStrategy)\n",
    "        \n",
    "        #Opponent Chooses scissors\n",
    "        if otherAction == actions - 1:\n",
    "            #Utility(Rock) = 1\n",
    "            actionUtility[0] = 1\n",
    "            #Utility(Paper) = -1\n",
    "            actionUtility[1] = -1\n",
    "        #Opponent Chooses Rock\n",
    "        elif otherAction == 0:\n",
    "            #Utility(Scissors) = -1\n",
    "            actionUtility[actions - 1] = -1\n",
    "            #Utility(Paper) = 1\n",
    "            actionUtility[1] = 1\n",
    "        #Opopnent Chooses Paper\n",
    "        else:\n",
    "            #Utility(Rock) = -1\n",
    "            actionUtility[0] = -1\n",
    "            #Utility(Scissors) = 1\n",
    "            actionUtility[2] = 1\n",
    "            \n",
    "        #Add the regrets from this decision\n",
    "        for i in range(0,actions):\n",
    "            regretSum[i] += actionUtility[i] - actionUtility[myaction]\n",
    "    return strategySum\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the average strategy\n",
    "- Returns the average strategy profile as each option divided by the total sum of all options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverageStrategy(iterations,oppStrategy):\n",
    "    actions = 3\n",
    "    strategySum = train(iterations,[0,0,0],oppStrategy)\n",
    "    avgStrategy = [0,0,0]\n",
    "    normalizingSum = 0\n",
    "    for i in range(0,actions):\n",
    "        normalizingSum += strategySum[i]\n",
    "    for i in range(0,actions):\n",
    "        if normalizingSum > 0:\n",
    "            avgStrategy[i] = strategySum[i] / normalizingSum\n",
    "        else:\n",
    "            avgStrategy[i] = 1.0 / actions\n",
    "    return avgStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the algorithm!\n",
    "* Demonstrates that we can generate a maximally exploitative strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opponent's Strategy [0.4, 0.3, 0.3]\n",
      "Maximally Exploitative Strat [6.666666666666666e-07, 0.999999, 3.333333333333333e-07]\n"
     ]
    }
   ],
   "source": [
    "oppStrat = [.4,.3,.3]\n",
    "print(\"Opponent's Strategy\",oppStrat)\n",
    "#train(1000000,[0,0,0],[.4,.3,.3])\n",
    "print(\"Maximally Exploitative Strat\", getAverageStrategy(1000000,oppStrat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have Both Agents Converge to Nash Equilibrium\n",
    "   * We will adapt our training algorithm to train two agents simulataneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two player training Function\n",
    "def train2Player(iterations,regretSum1,regretSum2,p2Strat):\n",
    "    ##Adapt Train Function for two players\n",
    "    actions = 3\n",
    "    actionUtility = [0,0,0]\n",
    "    strategySum1 = [0,0,0]\n",
    "    strategySum2 = [0,0,0]\n",
    "    for i in range(0,iterations):\n",
    "        \n",
    "        ##Retrieve Actions\n",
    "        t1 = getStrategy(regretSum1,strategySum1)\n",
    "        strategy1 = t1[0]\n",
    "        strategySum1 = t1[1]\n",
    "        myaction = getAction(strategy1)\n",
    "        t2 = getStrategy(regretSum2,p2Strat)\n",
    "        strategy2 = t2[0]\n",
    "        strategySum2 = t2[1]\n",
    "        otherAction = getAction(strategy2)\n",
    "        \n",
    "        #Opponent Chooses scissors\n",
    "        if otherAction == actions - 1:\n",
    "            #Utility(Rock) = 1\n",
    "            actionUtility[0] = 1\n",
    "            #Utility(Paper) = -1\n",
    "            actionUtility[1] = -1\n",
    "        #Opponent Chooses Rock\n",
    "        elif otherAction == 0:\n",
    "            #Utility(Scissors) = -1\n",
    "            actionUtility[actions - 1] = -1\n",
    "            #Utility(Paper) = 1\n",
    "            actionUtility[1] = 1\n",
    "        #Opopnent Chooses Paper\n",
    "        else:\n",
    "            #Utility(Rock) = -1\n",
    "            actionUtility[0] = -1\n",
    "            #Utility(Scissors) = 1\n",
    "            actionUtility[2] = 1\n",
    "            \n",
    "        #Add the regrets from this decision\n",
    "        for i in range(0,actions):\n",
    "            regretSum1[i] += actionUtility[i] - actionUtility[myaction]\n",
    "            regretSum2[i] += -(actionUtility[i] - actionUtility[myaction])\n",
    "            \n",
    "    return (strategySum1, strategySum2)\n",
    "\n",
    "#Returns a nash equilibrium reached by two opponents through CFRM\n",
    "def RPStoNash(iterations,oppStrat):\n",
    "    strats = train2Player(iterations,[0,0,0],[0,0,0],oppStrat)\n",
    "    s1 = sum(strats[0])\n",
    "    s2 = sum(strats[1])\n",
    "    for i in range(3):\n",
    "        if s1 > 0:\n",
    "            strats[0][i] = strats[0][i]/s1\n",
    "        if s2 > 0:\n",
    "            strats[1][i] = strats[1][i]/s2\n",
    "    return strats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.34083239238186, 0.3340920629119219, 0.3250755447062181], [0.32967926313477963, 0.33222032740623947, 0.3381004094589809])\n"
     ]
    }
   ],
   "source": [
    "print(RPStoNash(1000000,[.4,.3,.3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colonel Blotto Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colonel Blotto and his arch-enemy, Boba Fett, are at war. **Each commander has S soldiers in total**, and **each soldier can be assigned to one of N < S battlefields.** Naturally, these commanders do not communicate and hence direct their soldiers independently. **Any number of soldiers can be allocated to each battlefield, including zero.** A commander claims a battlefield **if they send more soldiers to the battlefield than their opponent.** The commander’s job is to break down his pool of soldiers into groups to which he assigned to each battlefield. **The winning commander is the one who claims the most battlefields.** For example, with (S,N) = (10,4) a Colonel Blotto may choose to play (2,2,2,4) while Boba Fett may choose to play (8,1,1,0). In this case, Colonel Blotto would win by claiming three of the four battlefields. **The war ends in a draw if both commanders claim the same number of battlefields.**\n",
    "\n",
    "Write a program where each player alternately uses regret-matching to find a Nash equilibrium for this game with **S = 5 and N = 3.** Some advice: before starting the training iterations, first think about all the valid pure strategies for one player; then, assign each pure strategy an ID number. Pure strategies can be represented as strings, objects, or 3-digit numbers: make a global array of these pure strategies whose indices refer to the ID of the strategy. Then, make a separate function that returns the utility of the one of the players given the IDs of the strategies used by each commander."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColonelBlottoTrainer:\n",
    "    def __init__(self):\n",
    "        #soldiers\n",
    "        s = 5\n",
    "        #Battle fields\n",
    "        n = 3\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kuhn Poker Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Die vs 1-Die Dudo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liar Die"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* [1] Introduction to Counterfactual Regret Minimization: http://modelai.gettysburg.edu/2013/cfr/cfr.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([0,0,0],1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
